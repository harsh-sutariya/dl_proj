{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataset import create_wall_dataloader\n",
    "import wandb\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=64, patch_size=8, in_chans=2, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # B, E, H/P, W/P\n",
    "        x = x.flatten(2).transpose(1, 2)  # B, N, E\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, mlp_ratio=4., drop=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=drop, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(int(dim * mlp_ratio), dim),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(*[self.norm1(x)]*3)[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, img_size=64, patch_size=8, in_chans=2, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim) for _ in range(6)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)  # B, N, E\n",
    "        \n",
    "        # Add cls token and pos embed\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]  # Return CLS token only\n",
    "\n",
    "class EncoderTrainer(nn.Module):\n",
    "    def __init__(self, device=\"cuda\", embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder = ViTEncoder(embed_dim=embed_dim)\n",
    "        self.target_encoder = ViTEncoder(embed_dim=embed_dim)\n",
    "        \n",
    "        # Freeze target encoder\n",
    "        for param in self.target_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Initialize target encoder\n",
    "        self.momentum_update(m=0.0)\n",
    "        \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def momentum_update(self, m=0.996):\n",
    "        for param_q, param_k in zip(self.encoder.parameters(),\n",
    "                                  self.target_encoder.parameters()):\n",
    "            param_k.data = param_k.data * m + param_q.data * (1. - m)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        p = self.predictor(z)\n",
    "        return p\n",
    "    \n",
    "    def compute_loss(self, states, debug=False):\n",
    "        # Get online predictions\n",
    "        z1 = self.encoder(states[:, 0])  # First frame\n",
    "        z2 = self.encoder(states[:, 1])  # Second frame\n",
    "        p1 = self.predictor(z1)\n",
    "        p2 = self.predictor(z2)\n",
    "        \n",
    "        # Get target projections\n",
    "        with torch.no_grad():\n",
    "            t1 = self.target_encoder(states[:, 0])\n",
    "            t2 = self.target_encoder(states[:, 1])\n",
    "        \n",
    "        # Normalize\n",
    "        p1 = F.normalize(p1, dim=-1)\n",
    "        p2 = F.normalize(p2, dim=-1)\n",
    "        t1 = F.normalize(t1, dim=-1)\n",
    "        t2 = F.normalize(t2, dim=-1)\n",
    "        \n",
    "        # BYOL-style loss\n",
    "        loss = 2 - 2 * (\n",
    "            (p1 * t2).sum(dim=-1).mean() +\n",
    "            (p2 * t1).sum(dim=-1).mean()\n",
    "        )\n",
    "        \n",
    "        if debug:\n",
    "            with torch.no_grad():\n",
    "                print(f\"\\nLoss: {loss.item():.4f}\")\n",
    "                print(f\"Prediction norm: {p1.norm(dim=1).mean():.4f}\")\n",
    "                print(f\"Target norm: {t1.norm(dim=1).mean():.4f}\")\n",
    "                print(f\"Cosine sim: {F.cosine_similarity(p1, t2).mean():.4f}\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def train_encoder():\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"vit-encoder-training\", config={\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"weight_decay\": 0.05,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 100,\n",
    "        \"scheduler\": \"CosineAnnealingLR\"\n",
    "    })\n",
    "\n",
    "    # Create dataloader using the provided function\n",
    "    train_loader = create_wall_dataloader(\n",
    "        data_path=\"/scratch/DL24FA/train\",\n",
    "        probing=False,\n",
    "        device=\"cuda\",\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        train=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    model = EncoderTrainer().cuda()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=wandb.config.learning_rate, weight_decay=wandb.config.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=wandb.config.epochs)\n",
    "    \n",
    "    # Log the model architecture\n",
    "    wandb.watch(model.encoder, log=\"all\", log_freq=10)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(wandb.config.epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = model.compute_loss(batch.states, debug=(epoch % 10 == 0))\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            model.momentum_update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Log batch loss\n",
    "            wandb.log({\"batch_loss\": loss.item()})\n",
    "        \n",
    "        # Print and log epoch statistics\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch}, Average Loss: {avg_loss:.4f}\")\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"avg_loss\": avg_loss,\n",
    "            \"learning_rate\": scheduler.get_last_lr()[0]\n",
    "        })\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Save the trained encoder\n",
    "    torch.save(model.encoder.state_dict(), 'encoder.pth')\n",
    "    wandb.save('encoder.pth')\n",
    "    \n",
    "    # Close wandb run\n",
    "    wandb.finish()\n",
    "    \n",
    "    return model.encoder\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    encoder = train_encoder()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
